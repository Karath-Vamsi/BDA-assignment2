# Big Data Analytics - Assignment 2

## ğŸ”§ Environment
- **Platform:** Google Colab
- **Language:** Python 3
- **Framework:** Apache Spark (via PySpark)
- **Setup:**
  - Java JDK 11
  - Spark 3.4.1
  - `pyspark`, `findspark`

---

## ğŸ“ Project Structure
```
assignment/
â”‚
â”œâ”€â”€ classification_model.ipynb      # Spark-based classification model
â”œâ”€â”€ clustering_model.ipynb          # Spark-based clustering model
â””â”€â”€ recommendation_engine.ipynb     # Spark-based movie recommendation system
```

---

## 1âƒ£ Classification Model

### âœ” Objective:
Build a classification model using Apache Spark to predict a target class based on input features.

### ğŸ“Š Dataset:
Used the **Iris dataset**, a classic multiclass classification problem with features like petal/sepal length & width.

### ğŸ“ˆ Model:
- **Algorithm:** Logistic Regression
- **Pipeline:** StringIndexer â†’ VectorAssembler â†’ LogisticRegression
- **Evaluation:** Accuracy, Confusion Matrix

### ğŸ” Result:
Achieved high accuracy (~95%+) on test data.

---

## 2âƒ£ Clustering Model

### âœ” Objective:
Perform unsupervised clustering on a dataset using Sparkâ€™s MLlib.

### ğŸ“Š Dataset:
Used the **Iris dataset** again, but this time without labels.

### ğŸ“ˆ Model:
- **Algorithm:** KMeans Clustering
- **Pipeline:** VectorAssembler â†’ KMeans
- **Evaluation:** Silhouette Score

### ğŸ” Result:
The clustering grouped the dataset into 3 distinct clusters (aligned well with actual classes).

---

## 3âƒ£ Recommendation Engine

### âœ” Objective:
Create a collaborative filtering recommendation system to suggest movies to users.

### ğŸ“Š Dataset:
**MovieLens 100k** dataset (user ratings for movies)

### ğŸ“ˆ Model:
- **Algorithm:** Alternating Least Squares (ALS)
- **Process:**
  - Data cleaning and transformation
  - Training ALS model
  - Generating recommendations for users

### ğŸ” Result:
Successfully generated top-N movie recommendations for individual users.

---

## âš™ï¸ Setup Instructions (for all notebooks)
Paste and run the following in a Colab cell to install Spark:
```python
!apt-get install openjdk-11-jdk -y
!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz
!tar -xzf spark-3.4.1-bin-hadoop3.tgz
!pip install -q pyspark
```

Then initialize Spark:
```python
import os
from pyspark.sql import SparkSession

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"
os.environ["PATH"] += f":{os.environ['SPARK_HOME']}/bin"

spark = SparkSession.builder.appName("MySparkApp").getOrCreate()
```

---

## âœ… Summary

| Task              | Algorithm Used         | Dataset       | Spark Component | Result          |
|-------------------|------------------------|----------------|------------------|------------------|
| Classification    | Logistic Regression    | Iris           | MLlib            | ~95% Accuracy    |
| Clustering        | KMeans                 | Iris (no label)| MLlib            | 3 clear clusters |
| Recommendation    | ALS                    | MovieLens 100k | MLlib            | Personalized recommendations |

---

Let me know if you want a PDF version of this README or to include visuals!

